{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA 3 - Linear Classification with Perceptrons and Logistic Regression\n",
    "\n",
    "Name: Viswesh Uppalapati \n",
    "\n",
    "PID: A15600068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3.1 Perceptron Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # of Passes | 2 | 3 | 4 |\n",
    "| :- | :- | :- | :- |\n",
    "| Training Error | 0.03761467889908257 | 0.02018348623853211 | 0.01926605504587156 |\n",
    "| Testing Error | 0.0610079575596817 | 0.04509283819628647 | 0.04509283819628647 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3.2 Logistic Regression Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Iters of GD Algorithm | 10 | 50 | 100 |\n",
    "| :- | :- | :- | :- |\n",
    "| Training Error | 0.29541284403669726 | 0.03853211009174312 | 0.01926605504587156 |\n",
    "| Testing Error | 0.29708222811671087 | 0.0610079575596817 | 0.04509283819628647 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3.3 Words Associated Most with Positive and Negative Classes - Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words associated most with the positive class for the perceptron algorithm are: file, program, line\n",
    "\n",
    "Words associated most with the negative class for the perceptron algorithm are: he, team, game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3.4 Words Associated with Most Positive and Negative Classes - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words associated most with the positive class for Logistic Regression are: window, file, use\n",
    "\n",
    "Words associated most with the positive class for Logistic Regression are: he, game, they"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3.5 Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is displayed as output in the last cell of this notebook.\n",
    "\n",
    "a) The highest accuracy for examples belong to class 5. (i and j = 5)\n",
    "\n",
    "b) The lowest accuracy for examples belong to class 3. (i and j = 3)\n",
    "\n",
    "c) Examples of class 6 are most often mistakenly classified as belonging to class 5. (i=5 and j=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>ndet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>ucontext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>newmask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>weick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>retinol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0    resources\n",
       "1         last\n",
       "2           of\n",
       "3      freedom\n",
       "4         from\n",
       "..         ...\n",
       "814       ndet\n",
       "815   ucontext\n",
       "816    newmask\n",
       "817      weick\n",
       "818    retinol\n",
       "\n",
       "[819 rows x 1 columns]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in the train data, test data, and the word-key dictionary that maps a\n",
    "# word to each entry of an x-feature vector by index\n",
    "\n",
    "train_data = pd.read_csv('pa3train.txt', sep = \" \", header = None)\n",
    "test_data = pd.read_csv('pa3test.txt', sep = \" \", header = None)\n",
    "word_dict = pd.read_csv('pa3dictionary.txt', sep = \" \", header = None).drop(1, axis = 1)\n",
    "word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting and splitting the data into train and test sets for labels 1 and 2\n",
    "\n",
    "dim_features = 819\n",
    "\n",
    "train_temp = train_data[(train_data[dim_features] == 1) | (train_data[dim_features] == 2)]\n",
    "test_temp = test_data[(test_data[dim_features] == 1) | (test_data[dim_features] == 2)]\n",
    "\n",
    "X_train = train_temp.drop(dim_features, axis = 1).to_numpy()\n",
    "Y_train = train_temp[dim_features].to_numpy()\n",
    "\n",
    "X_test = test_temp.drop(dim_features, axis = 1).to_numpy()\n",
    "Y_test = test_temp[dim_features].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps label 2 to -1 and label 1 to 1 for train and test set\n",
    "\n",
    "Y_train = np.array(list(map(lambda x: -1 if x == 2 else 1, Y_train)))\n",
    "Y_test = np.array(list(map(lambda x: -1 if x == 2 else 1, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes yt * <wt, xt> for the perceptron algorithm check\n",
    "def check_dot(x_i, y_i, w_i):\n",
    "    return y_i * (w_i @ x_i)\n",
    "\n",
    "# This function passes through the perceptron algorithm once\n",
    "def one_pass_perceptron(X_data, Y_data, w):\n",
    "    \n",
    "    # Loop through all the X_data that is passed in order\n",
    "    for i in range(len(X_data)):\n",
    "        \n",
    "        # save the old w and get the current y label and x vector\n",
    "        temp = w\n",
    "        x_i = X_data[i, :]\n",
    "        y_i = Y_data[i]\n",
    "        \n",
    "        # perform the perceptron check and update w accordingly\n",
    "        if check_dot(x_i, y_i, w) <= 0:\n",
    "            w = temp + y_i*x_i\n",
    "        else:\n",
    "            w = temp\n",
    "    \n",
    "    # return w after one pass through the algorithm\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the y_label for one x vector\n",
    "def predict_perceptron(w, x_i):\n",
    "    dot = w @ x_i\n",
    "    \n",
    "    # check the dot product and return label accordingly\n",
    "    # if the dot product is 0, randomly tie break\n",
    "    if dot > 0:\n",
    "        return 1\n",
    "    elif dot < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return np.random.choice(a = [1, -1], size = 1)\n",
    "\n",
    "# This function uses the above function to predict for an entire set\n",
    "# of X_data and return the corresponding labels based on w provided\n",
    "def predict(w, X_data):\n",
    "    return np.array(list(map(lambda x: predict_perceptron(w, x), X_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial w_0 is the zero vector with the same dimension as the x-vectors\n",
    "w_0 = np.zeros(dim_features)\n",
    "\n",
    "# call one pass of the perceptron alg and store the resulting w\n",
    "w_one_pass = one_pass_perceptron(X_train, Y_train, w_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04036697247706422"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training error check after one pass of alg, should be ~ 0.04\n",
    "one_pass_preds = predict(w_one_pass, X_train)\n",
    "one_pass_error = np.mean(one_pass_preds != Y_train)\n",
    "one_pass_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error after Two Passes of Perceptron Algorithm: 0.03761467889908257\n",
      "Testing Error after Two Passes of Perceptron Algorithm: 0.0610079575596817\n"
     ]
    }
   ],
   "source": [
    "# Train and Test errors after two passes of the perceptron algorithm\n",
    "\n",
    "w_two_pass = one_pass_perceptron(X_train, Y_train, w_one_pass)\n",
    "\n",
    "train_preds = predict(w_two_pass, X_train)\n",
    "test_preds = predict(w_two_pass, X_test)\n",
    "\n",
    "train_error = np.mean(train_preds != Y_train)\n",
    "test_error = np.mean(test_preds != Y_test)\n",
    "\n",
    "print(\"Training Error after Two Passes of Perceptron Algorithm: \" + str(train_error))\n",
    "print(\"Testing Error after Two Passes of Perceptron Algorithm: \" + str(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error after Three Passes of Perceptron Algorithm: 0.02018348623853211\n",
      "Testing Error after Three Passes of Perceptron Algorithm: 0.04509283819628647\n"
     ]
    }
   ],
   "source": [
    "# Train and Test errors after three passes of the perceptron algorithm\n",
    "\n",
    "w_three_pass = one_pass_perceptron(X_train, Y_train, w_two_pass)\n",
    "\n",
    "train_preds = predict(w_three_pass, X_train)\n",
    "test_preds = predict(w_three_pass, X_test)\n",
    "\n",
    "train_error = np.mean(train_preds != Y_train)\n",
    "test_error = np.mean(test_preds != Y_test)\n",
    "\n",
    "print(\"Training Error after Three Passes of Perceptron Algorithm: \" + str(train_error))\n",
    "print(\"Testing Error after Three Passes of Perceptron Algorithm: \" + str(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error after Four Passes of Perceptron Algorithm: 0.01926605504587156\n",
      "Testing Error after Four Passes of Perceptron Algorithm: 0.04509283819628647\n"
     ]
    }
   ],
   "source": [
    "# Train and Test errors after three passes of the perceptron algorithm\n",
    "\n",
    "w_four_pass = one_pass_perceptron(X_train, Y_train, w_three_pass)\n",
    "\n",
    "train_preds = predict(w_four_pass, X_train)\n",
    "test_preds = predict(w_four_pass, X_test)\n",
    "\n",
    "train_error = np.mean(train_preds != Y_train)\n",
    "test_error = np.mean(test_preds != Y_test)\n",
    "\n",
    "print(\"Training Error after Four Passes of Perceptron Algorithm: \" + str(train_error))\n",
    "print(\"Testing Error after Four Passes of Perceptron Algorithm: \" + str(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates one component of the gradient of the logistic regression\n",
    "# loss function\n",
    "def gradient_calc(x_i, y_i, w_i):\n",
    "    return (y_i * x_i) / (1 + np.exp(y_i*(w_i @ x_i)))\n",
    "\n",
    "# This function runs the gradient descent algorithm based on a given number\n",
    "# of iteration\n",
    "def logistic_regression(X_data, Y_data, w, num_iterations):\n",
    "    \n",
    "    # initialize learning rate\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # runs for num_iterations steps of gradient descent\n",
    "    for a in range(num_iterations):\n",
    "        gradient = 0\n",
    "        temp = w\n",
    "        \n",
    "        # calculate the gradient based on X_data\n",
    "        for i in range(len(X_data)):\n",
    "            \n",
    "            x_i = X_data[i]\n",
    "            y_i = Y_data[i]\n",
    "            \n",
    "            gradient += gradient_calc(x_i, y_i, w)\n",
    "        \n",
    "        # update step for w based on learning rate\n",
    "        w = temp + learning_rate * gradient\n",
    "            \n",
    "    # return final w after given number of iterations\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts for one x-feature vector based on given w\n",
    "def predict_log_reg(w, x_i):\n",
    "    # check the value of the conditional probability\n",
    "    check = 1 / (1 + np.exp(-1 * w @ x_i))\n",
    "    \n",
    "    # Return the corresponding label based on value of\n",
    "    # check, if it's exactly 0.5, tie break randomly\n",
    "    if check > 0.5:\n",
    "        return 1\n",
    "    elif check < 0.5:\n",
    "        return -1\n",
    "    else:\n",
    "        return np.random.choice(a = [1, -1], size = 1)\n",
    "        \n",
    "\n",
    "# This function applies predict_log_reg to every x-feature vector\n",
    "# and returns predictions\n",
    "def predict_logistic(X_data, w):\n",
    "    return np.array(list(map(lambda x: predict_log_reg(w, x), X_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline check of 2 gradient descent iterations on w starting at zero vector\n",
    "\n",
    "w_0 = np.zeros(819)\n",
    "w_2_iters = logistic_regression(X_train, Y_train, w_0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-472-f9e246a71ebe>:4: RuntimeWarning: overflow encountered in exp\n",
      "  check = 1 / (1 + np.exp(-1 * w @ x_i))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4954128440366973"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline check for 2 iterations of gradient descent alg, train erro\n",
    "# should be ~ 0.497\n",
    "\n",
    "train_preds = predict_logistic(X_train, w_2_iters)\n",
    "np.mean(train_preds != Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-471-20bbb644fb5a>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return (y_i * x_i) / (1 + np.exp(y_i*(w_i @ x_i)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error after 10 iterations of gradient descent algorithm: 0.29541284403669726\n",
      "Testing Error after 10 iterations of gradient descent algorithm: 0.29708222811671087\n"
     ]
    }
   ],
   "source": [
    "# Train and Test errors after 10 iterations of gradient descent algorithm\n",
    "\n",
    "w_10_iters = logistic_regression(X_train, Y_train, w_0, 10)\n",
    "\n",
    "train_preds = predict_logistic(X_train, w_10_iters)\n",
    "test_preds = predict_logistic(X_test, w_10_iters)\n",
    "\n",
    "train_error = np.mean(train_preds != Y_train)\n",
    "test_error = np.mean(test_preds != Y_test)\n",
    "\n",
    "print(\"Training Error after 10 iterations of gradient descent algorithm: \" + str(train_error))\n",
    "print(\"Testing Error after 10 iterations of gradient descent algorithm: \" + str(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-471-20bbb644fb5a>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return (y_i * x_i) / (1 + np.exp(y_i*(w_i @ x_i)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error after 50 iterations of gradient descent algorithm: 0.03853211009174312\n",
      "Testing Error after 50 iterations of gradient descent algorithm: 0.0610079575596817\n"
     ]
    }
   ],
   "source": [
    "# Train and Test errors after 50 iterations of gradient descent algorithm\n",
    "\n",
    "w_50_iters = logistic_regression(X_train, Y_train, w_0, 50)\n",
    "\n",
    "train_preds = predict_logistic(X_train, w_50_iters)\n",
    "test_preds = predict_logistic(X_test, w_50_iters)\n",
    "\n",
    "train_error = np.mean(train_preds != Y_train)\n",
    "test_error = np.mean(test_preds != Y_test)\n",
    "\n",
    "print(\"Training Error after 50 iterations of gradient descent algorithm: \" + str(train_error))\n",
    "print(\"Testing Error after 50 iterations of gradient descent algorithm: \" + str(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-471-20bbb644fb5a>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return (y_i * x_i) / (1 + np.exp(y_i*(w_i @ x_i)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error after 100 iterations of gradient descent algorithm: 0.01926605504587156\n",
      "Testing Error after 100 iterations of gradient descent algorithm: 0.04509283819628647\n"
     ]
    }
   ],
   "source": [
    "# Train and Test errors after 100 iterations of gradient descent algorithm\n",
    "\n",
    "w_100_iters = logistic_regression(X_train, Y_train, w_0, 100)\n",
    "\n",
    "train_preds = predict_logistic(X_train, w_100_iters)\n",
    "test_preds = predict_logistic(X_test, w_100_iters)\n",
    "\n",
    "train_error = np.mean(train_preds != Y_train)\n",
    "test_error = np.mean(test_preds != Y_test)\n",
    "\n",
    "print(\"Training Error after 100 iterations of gradient descent algorithm: \" + str(train_error))\n",
    "print(\"Testing Error after 100 iterations of gradient descent algorithm: \" + str(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Anaylsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most associated with positive class: ['file', 'program', 'line']\n",
      "Words most associated with negative class: ['he', 'team', 'game']\n"
     ]
    }
   ],
   "source": [
    "# Words most associated with the positive and negative class for w given by perceptron\n",
    "# algorithm after 3 passes\n",
    "\n",
    "# combine the word_dict with w indexwise\n",
    "w_data = pd.DataFrame(w_three_pass)\n",
    "perceptron_words = w_data.merge(word_dict, on =  word_dict.index).drop('key_0', axis = 1)\n",
    "\n",
    "# find most positive and most negative w-values by sorting and print words\n",
    "most_positive = perceptron_words.sort_values('0_x', ascending = False)\n",
    "most_negative = perceptron_words.sort_values('0_x', ascending = True)\n",
    "print(\"Words most associated with positive class: \" + str(list(most_positive.iloc[0:3]['0_y'])))\n",
    "print(\"Words most associated with negative class: \" + str(list(most_negative.iloc[0:3]['0_y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most associated with positive class: ['window', 'file', 'use']\n",
      "Words most associated with negative class: ['he', 'game', 'they']\n"
     ]
    }
   ],
   "source": [
    "# Words most associated with the positive and negative class for w given by logistic regression\n",
    "# algorithm after 50 iteration\n",
    "\n",
    "# combine the word_dict with w indexwise\n",
    "w_data = pd.DataFrame(w_50_iters)\n",
    "log_reg_words = w_data.merge(word_dict, on =  word_dict.index).drop('key_0', axis = 1)\n",
    "\n",
    "# find most positive and most negative w-values by sorting and print words\n",
    "most_positive = log_reg_words.sort_values('0_x', ascending = False)\n",
    "most_negative = log_reg_words.sort_values('0_x', ascending = True)\n",
    "print(\"Words most associated with positive class: \" + str(list(most_positive.iloc[0:3]['0_y'])))\n",
    "print(\"Words most associated with negative class: \" + str(list(most_negative.iloc[0:3]['0_y'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One vs. All Classifier and Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps 1 to the provided label, all other labels are mapped to -1\n",
    "def partition_mapper(label, y_data):\n",
    "    return np.array(list(map(lambda x: 1 if x == label else -1, y_data)))\n",
    "\n",
    "# computes the list of one vs all classifier\n",
    "def one_v_all(X_data, Y_data):\n",
    "    classifiers = []\n",
    "    num_labels = 6\n",
    "    \n",
    "    # call one pass of perceptron on each label to build\n",
    "    # each classifier\n",
    "    for i in range(num_labels):\n",
    "        label = i + 1\n",
    "        w_0 = np.zeros(len(X_data[0]))\n",
    "        y = partition_mapper(label, Y_data)\n",
    "        C_i = one_pass_perceptron(X_data, y, w_0)\n",
    "        classifiers.append(C_i)\n",
    "    \n",
    "    # return the list of classifiers\n",
    "    return classifiers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process training data and compute one vs all classifiers\n",
    "\n",
    "X_one_v_all = train_data.drop(819, axis = 1).to_numpy()\n",
    "Y_one_v_all = train_data[819].to_numpy()\n",
    "classifiers = one_v_all(X_one_v_all, Y_one_v_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts using the list of classifiers for one point\n",
    "def predict_one_v_all(classifiers, x):\n",
    "    # compute <w, x> for each w in classifiers \n",
    "    # through matrix multiplication\n",
    "    checks = classifiers @ x\n",
    "    \n",
    "    count = 0\n",
    "    result = 0\n",
    "    \n",
    "    # output the label of the one vs all classifier\n",
    "    for x in range(len(checks)):\n",
    "        label = x + 1\n",
    "        \n",
    "        if checks[x] > 0:\n",
    "            count += 1\n",
    "            result = label\n",
    "    \n",
    "    # if exactly one label had <w, x> > 0, return\n",
    "    # that label, otherwise prediction is 0 (undefined/unsure)\n",
    "    if count == 1:\n",
    "        return result\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# predicts the label for each X-vector by using predict_one_v_all\n",
    "def predict_all(classifiers, X_data):\n",
    "    return np.array(list(map(lambda x: predict_one_v_all(classifiers, x), X_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the confusion matrix of one vs all classifier on test data\n",
    "def confusion_matrix(classifiers, X_data, Y_data):\n",
    "    mat = np.zeros([7, 6])\n",
    "    # get all the predictions\n",
    "    preds = predict_all(classifiers, X_data)\n",
    "    \n",
    "    # compute the matrix\n",
    "    for i in range(len(mat)):\n",
    "        check = i + 1\n",
    "        if i == 6:\n",
    "            check = 0\n",
    "        for j in range(len(mat[0])):\n",
    "            label = j + 1\n",
    "            n_j = len(Y_test_one_v_all[Y_test_one_v_all == label])\n",
    "            count = 0\n",
    "            \n",
    "            # compute each entry based on the preds that were\n",
    "            # label as i, when they were supposed to be j\n",
    "            for x in range(len(Y_data)):\n",
    "                if (preds[x] == check) and (Y_test_one_v_all[x] == label):\n",
    "                    count += 1\n",
    "\n",
    "            # each entry is the proportion of mistakes made by the classifier\n",
    "            mat[i][j] = count / n_j\n",
    "    \n",
    "    # return the confusion matrix\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71891892 0.00520833 0.03428571 0.02173913 0.         0.        ]\n",
      " [0.01081081 0.65625    0.03428571 0.02717391 0.01282051 0.01851852]\n",
      " [0.         0.015625   0.37142857 0.         0.         0.02777778]\n",
      " [0.01621622 0.00520833 0.         0.69021739 0.         0.        ]\n",
      " [0.01621622 0.03125    0.07428571 0.00543478 0.80128205 0.12037037]\n",
      " [0.00540541 0.01041667 0.03428571 0.         0.07051282 0.49074074]\n",
      " [0.23243243 0.27604167 0.45142857 0.25543478 0.11538462 0.34259259]]\n"
     ]
    }
   ],
   "source": [
    "# Process test data and construct the confusion matrix\n",
    "\n",
    "X_test_one_v_all = test_data.drop(819, axis = 1).to_numpy()\n",
    "Y_test_one_v_all = test_data[819].to_numpy()\n",
    "conf_mat = confusion_matrix(classifiers, X_test_one_v_all, Y_test_one_v_all)\n",
    "print(conf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
